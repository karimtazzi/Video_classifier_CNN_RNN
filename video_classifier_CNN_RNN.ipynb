{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64a42a09",
      "metadata": {
        "id": "64a42a09"
      },
      "source": [
        "# Video Segementation and Classification Using CNN & RNN "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Dans la classification d'images :** nous prenons des images, utilisons des extracteurs de caractéristiques (les réseaux de neurones convolutifs ou CNN) pour extraire les caractéristiques des images, puis classifions cette image en fonction de ces caractéristiques extraites. La classification vidéo implique une seule étape supplémentaire.\n",
        "\n",
        "## **Au niveau de la classification vidéo :**\n",
        "**Les vidéos** sont une collection d'images (Frames) disposées dans un ordre spécifique.\n",
        "\n",
        "**1-** Nous extrayons d'abord les images de la vidéo donnée.\n",
        "\n",
        "**2-** utiliser des extracteurs de caractéristiques ( les réseaux de     neurones convolutifs ou CNN) pour extraire les caractéristiques des frames\n",
        "\n",
        "**3-** Classer chaque image en fonction de ces caractéristiques extraites.\n"
      ],
      "metadata": {
        "id": "TAv1ffiiwttv"
      },
      "id": "TAv1ffiiwttv"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "1qcNaZKL7rWq",
        "outputId": "ed6530d5-c7d1-4be9-dffb-66f4ca61ab10"
      },
      "id": "1qcNaZKL7rWq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a624212d-5bd9-4bab-b69a-776574c7a645\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a624212d-5bd9-4bab-b69a-776574c7a645\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "dataset_path = os.listdir('dataset/train')\n",
        "\n",
        "label_types = os.listdir('dataset/train')\n",
        "print (label_types)  "
      ],
      "metadata": {
        "id": "B3cWAhBa-sk5"
      },
      "id": "B3cWAhBa-sk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "935f06c4",
      "metadata": {
        "id": "935f06c4"
      },
      "source": [
        "# data training preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e00542",
      "metadata": {
        "id": "d7e00542",
        "outputId": "b84a46d7-b6c0-40e3-ada4-fb73163c6847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       tag                             video_name\n",
            "0  dancing   dataset/train/dancing/dancin (1).mp4\n",
            "1  dancing  dataset/train/dancing/dancin (10).mp4\n",
            "2  dancing  dataset/train/dancing/dancin (11).mp4\n",
            "3  dancing  dataset/train/dancing/dancin (12).mp4\n",
            "4  dancing  dataset/train/dancing/dancin (13).mp4\n",
            "      tag                             video_name\n",
            "140  yoga  dataset/train/yoga/yoga_asana (5).mp4\n",
            "141  yoga  dataset/train/yoga/yoga_asana (6).mp4\n",
            "142  yoga  dataset/train/yoga/yoga_asana (7).mp4\n",
            "143  yoga  dataset/train/yoga/yoga_asana (8).mp4\n",
            "144  yoga  dataset/train/yoga/yoga_asana (9).mp4\n"
          ]
        }
      ],
      "source": [
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('dataset/train' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(train_df.head())\n",
        "print(train_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ac79ad",
      "metadata": {
        "id": "27ac79ad"
      },
      "outputs": [],
      "source": [
        "df = train_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0897c2",
      "metadata": {
        "id": "ee0897c2"
      },
      "source": [
        "# Test Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413b31f2",
      "metadata": {
        "id": "413b31f2",
        "outputId": "7f33da8e-7d8d-4e1e-b7c9-06be5eba72d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dancing', 'exercise', 'yoga']\n",
            "Types of activities found:  3\n",
            "       tag                             video_name\n",
            "0  dancing  dataset/test/dancing/dancing (21).mp4\n",
            "1  dancing  dataset/test/dancing/dancing (22).mp4\n",
            "2  dancing   dataset/test/dancing/dancing (23.mp4\n",
            "3  dancing  dataset/test/dancing/dancing (24).mp4\n",
            "4  dancing  dataset/test/dancing/dancing (25).mp4\n",
            "     tag                       video_name\n",
            "17  yoga  dataset/test/yoga/yoga (24).mp4\n",
            "18  yoga  dataset/test/yoga/yoga (25).mp4\n",
            "19  yoga  dataset/test/yoga/yoga (26).mp4\n",
            "20  yoga  dataset/test/yoga/yoga (27).mp4\n",
            "21  yoga  dataset/test/yoga/yoga (28).mp4\n"
          ]
        }
      ],
      "source": [
        "dataset_path = os.listdir('dataset/test')\n",
        "print(dataset_path)\n",
        "\n",
        "room_types = os.listdir('dataset/test')\n",
        "print(\"Types of activities found: \", len(dataset_path))\n",
        "\n",
        "rooms = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_rooms = os.listdir('dataset/test' + '/' +item)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_rooms:\n",
        "    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))\n",
        "    \n",
        "# Build a dataframe        \n",
        "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "\n",
        "df = test_df.loc[:,['video_name','tag']]\n",
        "df\n",
        "df.to_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918d9338",
      "metadata": {
        "id": "918d9338"
      },
      "outputs": [],
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4fed40c",
      "metadata": {
        "id": "f4fed40c"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
        "  except RuntimeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bdf44bb",
      "metadata": {
        "id": "2bdf44bb"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b716cfb",
      "metadata": {
        "id": "8b716cfb",
        "outputId": "a3c0fc96-7737-43ec-81d2-d23ae844530c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos for training: 145\n",
            "Total videos for testing: 22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>115</td>\n",
              "      <td>dataset/train/yoga/yoga (5).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>84</td>\n",
              "      <td>dataset/train/exercise/exercise (8).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>140</td>\n",
              "      <td>dataset/train/yoga/yoga_asana (5).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>dataset/train/yoga/yoga (11).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>117</td>\n",
              "      <td>dataset/train/yoga/yoga (7).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>85</td>\n",
              "      <td>dataset/train/exercise/exercise (9).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>dataset/train/exercise/exercis (3).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>118</td>\n",
              "      <td>dataset/train/yoga/yoga (8).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>72</td>\n",
              "      <td>dataset/train/exercise/exercise (19).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>127</td>\n",
              "      <td>dataset/train/yoga/yoga_asana (16).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0                                video_name       tag\n",
              "115         115           dataset/train/yoga/yoga (5).mp4      yoga\n",
              "84           84   dataset/train/exercise/exercise (8).mp4  exercise\n",
              "140         140     dataset/train/yoga/yoga_asana (5).mp4      yoga\n",
              "99           99          dataset/train/yoga/yoga (11).mp4      yoga\n",
              "117         117           dataset/train/yoga/yoga (7).mp4      yoga\n",
              "85           85   dataset/train/exercise/exercise (9).mp4  exercise\n",
              "55           55    dataset/train/exercise/exercis (3).mp4  exercise\n",
              "118         118           dataset/train/yoga/yoga (8).mp4      yoga\n",
              "72           72  dataset/train/exercise/exercise (19).mp4  exercise\n",
              "127         127    dataset/train/yoga/yoga_asana (16).mp4      yoga"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93c182a",
      "metadata": {
        "id": "b93c182a"
      },
      "source": [
        "# Feed the videos to a network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd68b72",
      "metadata": {
        "id": "fcd68b72"
      },
      "outputs": [],
      "source": [
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c243c6ee",
      "metadata": {
        "id": "c243c6ee"
      },
      "source": [
        "   ### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388c0afe",
      "metadata": {
        "id": "388c0afe"
      },
      "outputs": [],
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877ca626",
      "metadata": {
        "id": "877ca626"
      },
      "source": [
        "### Label Encoding\n",
        "StringLookup layer encode the class labels as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801339d8",
      "metadata": {
        "id": "801339d8",
        "outputId": "2d219bbd-f593-4612-ddd2-0be7434f0655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dancing', 'exercise', 'yoga']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2]], dtype=int64)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "labels = train_df[\"tag\"].values\n",
        "labels = label_processor(labels[..., None]).numpy()\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde4ac85",
      "metadata": {
        "id": "fde4ac85"
      },
      "source": [
        "Finally, we can put all the pieces together to create our data processing utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76216366",
      "metadata": {
        "id": "76216366"
      },
      "outputs": [],
      "source": [
        "#Define hyperparameters\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11befa62",
      "metadata": {
        "id": "11befa62",
        "outputId": "d2ecf1b5-dbef-4a18-ac98-057008353d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame features in train set: (145, 20, 2048)\n",
            "Frame masks in train set: (145, 20)\n",
            "train_labels in train set: (145, 1)\n",
            "test_labels in train set: (22, 1)\n"
          ]
        }
      ],
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    \n",
        "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
        "    labels = df[\"tag\"].values\n",
        "    \n",
        "    #convert classlabels to label encoding\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
        "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"train_labels in train set: {train_labels.shape}\")\n",
        "\n",
        "print(f\"test_labels in train set: {test_labels.shape}\")\n",
        "\n",
        "# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407e0827",
      "metadata": {
        "id": "407e0827"
      },
      "source": [
        "# The sequence model\n",
        "Now, we will feed this data to a sequence model consisting of recurrent layers like GRU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7382f3c1",
      "metadata": {
        "id": "7382f3c1",
        "outputId": "9b17d501-078d-4abe-9f24-54daa20a25d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - 7s 486ms/step - loss: 1.0980 - accuracy: 0.4950 - val_loss: 1.1039 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.10389, saving model to ./tmp\\video_classifier\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 1.0956 - accuracy: 0.4950 - val_loss: 1.1091 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.10389\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 1s 152ms/step - loss: 1.0933 - accuracy: 0.4950 - val_loss: 1.1142 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.10389\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 1.0913 - accuracy: 0.4950 - val_loss: 1.1193 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.10389\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 1.0890 - accuracy: 0.4950 - val_loss: 1.1242 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.10389\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 1.0870 - accuracy: 0.4950 - val_loss: 1.1290 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.10389\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 1.0849 - accuracy: 0.4950 - val_loss: 1.1340 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.10389\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 1.0829 - accuracy: 0.4950 - val_loss: 1.1388 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.10389\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 1.0809 - accuracy: 0.4950 - val_loss: 1.1436 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.10389\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 1.0790 - accuracy: 0.4950 - val_loss: 1.1484 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.10389\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 1s 158ms/step - loss: 1.0770 - accuracy: 0.4950 - val_loss: 1.1529 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.10389\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 1s 153ms/step - loss: 1.0751 - accuracy: 0.4950 - val_loss: 1.1575 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.10389\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 1.0732 - accuracy: 0.4950 - val_loss: 1.1622 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.10389\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 1s 158ms/step - loss: 1.0714 - accuracy: 0.4950 - val_loss: 1.1670 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.10389\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 1s 161ms/step - loss: 1.0695 - accuracy: 0.4950 - val_loss: 1.1717 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.10389\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 1.0677 - accuracy: 0.4950 - val_loss: 1.1763 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.10389\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 1s 162ms/step - loss: 1.0658 - accuracy: 0.4950 - val_loss: 1.1810 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.10389\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 1.0641 - accuracy: 0.4950 - val_loss: 1.1856 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.10389\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 1.0623 - accuracy: 0.4950 - val_loss: 1.1903 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.10389\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 1.0606 - accuracy: 0.4950 - val_loss: 1.1949 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.10389\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 1.0588 - accuracy: 0.4950 - val_loss: 1.1996 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.10389\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 1.0570 - accuracy: 0.4950 - val_loss: 1.2042 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.10389\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 1.0553 - accuracy: 0.4950 - val_loss: 1.2088 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.10389\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 1.0536 - accuracy: 0.4950 - val_loss: 1.2134 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.10389\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 1.0519 - accuracy: 0.4950 - val_loss: 1.2181 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.10389\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 1s 165ms/step - loss: 1.0503 - accuracy: 0.4950 - val_loss: 1.2227 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.10389\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 1.0485 - accuracy: 0.4950 - val_loss: 1.2273 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.10389\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 1s 153ms/step - loss: 1.0469 - accuracy: 0.4950 - val_loss: 1.2319 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.10389\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 1s 158ms/step - loss: 1.0453 - accuracy: 0.4950 - val_loss: 1.2365 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.10389\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 1s 152ms/step - loss: 1.0437 - accuracy: 0.4950 - val_loss: 1.2410 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.10389\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0992 - accuracy: 0.2273\n",
            "Test accuracy: 22.73%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "EPOCHS = 30\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"./tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1f1681",
      "metadata": {
        "id": "6f1f1681"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846a5956",
      "metadata": {
        "id": "846a5956",
        "outputId": "45c501d8-bd15-4236-d60d-142fc935fe25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test video path: dataset/test/dancing/dancing (23.mp4\n",
            "  dancing: 33.42%\n",
            "  exercise: 33.42%\n",
            "  yoga: 33.16%\n"
          ]
        }
      ],
      "source": [
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"test\", path))\n",
        "    frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "print(f\"Test video path: {test_video}\")\n",
        "\n",
        "test_frames = sequence_prediction(test_video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6309d87b",
      "metadata": {
        "id": "6309d87b",
        "outputId": "1872df23-71dc-4fa4-ccc8-7b7d319b3288"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
              "        <source src=\"dataset/test/dancing/dancing (23.mp4\" type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
              "    </video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
        "        <source src=\"dataset/test/dancing/dancing (23.mp4\" type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
        "    </video>\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6150d7e2",
      "metadata": {
        "id": "6150d7e2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b16e337",
      "metadata": {
        "id": "0b16e337"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad15f322",
      "metadata": {
        "id": "ad15f322"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "video_classifier_working.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}